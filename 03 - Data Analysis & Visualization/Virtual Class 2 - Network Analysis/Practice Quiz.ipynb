{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd7e91f6",
   "metadata": {},
   "source": [
    "# Practice Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d8d61",
   "metadata": {},
   "source": [
    "### (1) Which of the following is/are true about Principal Component Analysis (PCA)?\n",
    "\n",
    "- The Principal Components are Eigenvectors of the sample Covariance Matrix\n",
    "- PCA maximizes the Projection Variance\n",
    "- PCA minimizes the Projection Residuals\n",
    "- **<span style='color: red'>All the above<span>**\n",
    "    \n",
    "The principal components are eigenvectors of the sample covariance matrix: PCA involves finding the eigenvectors of the covariance matrix (or correlation matrix) of the data. These eigenvectors, known as principal components, represent the directions in the data with the highest variance.\n",
    "\n",
    "PCA maximizes the projection variance: The principal components are arranged in descending order of the amount of variance they capture. The first principal component captures the most variance, followed by the second, and so on. Therefore, PCA aims to maximize the variance along each principal component, enabling us to retain the most significant information in the data.\n",
    "\n",
    "PCA minimizes projection residuals: The projection residuals represent the difference between the original data and its projection onto the principal components. PCA minimizes the sum of the squared residuals, ensuring that the projection onto the principal components provides the best approximation of the original data.\n",
    "\n",
    "Hence, all three statements are true about Principal Components Analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc2f6c",
   "metadata": {},
   "source": [
    "### (2) In t-SNE objective function, which mathematical concept is used to measure the \"distance\" between distributions and minimize the discrepancy between high-dimensional and low-dimensional sample distributions?\n",
    "\n",
    "- **<span style='color: red'>Kullback-Leibler Divergence</span>**\n",
    "- Covariance Matrix\n",
    "- Eigenvalues\n",
    "- None of the above\n",
    "\n",
    "In the t-SNE (t-Distributed Stochastic Neighbor Embedding) objective function, the Kullback-Leibler (KL) divergence is specifically used to measure the \"distance\" between distributions and minimize the discrepancy between high-dimensional and low-dimensional sample distributions.\n",
    "\n",
    "The t-SNE algorithm aims to find a low-dimensional representation of high-dimensional data while preserving the local structure and relationships between data points. It achieves this by minimizing the KL divergence between two probability distributions: the pairwise similarities of data points in the original high-dimensional space and the pairwise similarities in the low-dimensional space.\n",
    "\n",
    "By minimizing the KL divergence, t-SNE ensures that similar data points are modeled by similar probability distributions in both the high-dimensional and low-dimensional spaces. This helps to reveal the underlying structure and clusters within the data, making t-SNE a powerful tool for visualization and exploratory analysis of complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5482303",
   "metadata": {},
   "source": [
    "### (3) How does t-SNE address the problem of crowding observed in Principal Component Analysis (PCA)?\n",
    "\n",
    "- By maximizing the projection residuals\n",
    "- By minimizing the projection variance\n",
    "- **<span style='color: red'>By introducing non-linearity in the embedding</span>**\n",
    "- By reducing the number of dimensions in the data\n",
    "\n",
    "t-SNE gives rise to a non-linear embedding where close-by points in the high-dimensional space remain close in the low-dimensional space. This non-linearity helps address the problem of crowding, which is often observed in PCA. Crowding refers to situations where points that are moderately distant in the high-dimensional space become too close together in the low-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9725e8b4",
   "metadata": {},
   "source": [
    "### (4) Which Centrality Measure captures the importance of a node's position in the network based on the number of connections it has?\n",
    "\n",
    "- **<span style='color: red'>Degree Centrality</span>**\n",
    "- Eigenvector Centrality\n",
    "- Closeness Centrality\n",
    "- Betweenness Centrality\n",
    "\n",
    "Degree centrality is a simple and intuitive measure that captures the importance of a node based on the number of connections it has in the network. Nodes with more connections are considered to have more influence and better access to information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128fe30",
   "metadata": {},
   "source": [
    "### (5) Which Centrality Measure quantifies the importance of a node based on the number of shortest paths that pass through it?\n",
    "\n",
    "- Degree Centrality\n",
    "- **<span style='color: red'>Betweenness Centrality</span>**\n",
    "- Closeness Centrality\n",
    "- None of the above\n",
    "\n",
    "Betweenness centrality measures the number of shortest paths passing through a node, indicating its potential control over the flow of information in a network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d2f290",
   "metadata": {},
   "source": [
    "### (6) In a Friendship Network, which Centrality Measure identifies the person who could best inform the group?\n",
    "\n",
    "- Degree Centrality\n",
    "- Eigenvector Centrality\n",
    "- **<span style='color: red'>Closeness Centrality</span>**\n",
    "- Betweenness Centrality\n",
    "\n",
    "In a friendship network, closeness centrality identifies the person who could best inform the group. Closeness centrality measures how close a person is to all other individuals in terms of the shortest paths. The person with high closeness centrality has direct access to information from various individuals and can efficiently disseminate it to the entire group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d530d",
   "metadata": {},
   "source": [
    "### (7) What approach does K-Means clustering algorithm use to overcome local optima while minimizing the error?\n",
    "\n",
    "- **<span style='color: red'>Randomly initializing the cluster centroids multiple times</span>**\n",
    "- Using a genetic algorithm to search for optimal cluster assignments\n",
    "- Incorporating gradient descent to escape local optima\n",
    "- Increasing the number of iterations to ensure convergence to global optima\n",
    "\n",
    "K-Means uses a greedy algorithm with random restarts to avoid local optima. The algorithm randomly initializes the cluster centroids multiple times and runs the optimization process independently for each initialization. By exploring different starting configurations, K-Means increases the chances of finding a better solution and escaping local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5791b55",
   "metadata": {},
   "source": [
    "### (8) What are the two main parameters used to define clusters using DBSCAN clustering algorithm?\n",
    "\n",
    "- Epsilon (radius of the neighborhood) and Mean\n",
    "- MinPts and MaxPts\n",
    "- **<span style='color: red'>MinPts and Epsilon (radius of the neighborhood)</span>**\n",
    "- None of the above\n",
    "\n",
    "DBSCAN uses the parameters MinPts (minimum number of points) and Epsilon (radius of the neighborhood) to define clusters. MinPts determines the minimum number of neighboring points a data point must have within the specified radius to be considered a core point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ab212",
   "metadata": {},
   "source": [
    "### (9) What is the main objective of K-Means clustering?\n",
    "\n",
    "- Maximizing the distances between samples within each cluster\n",
    "- **<span style='color: red'>Minimizing the sum of the pairwise distances between samples within each cluster</span>**\n",
    "- Minimizing the distances to the cluster centroids\n",
    "- Maximizing the sum of the distances between samples and clusters means\n",
    "\n",
    "The main objective of K-Means clustering is to minimize the sum of the pairwise distances between samples within each cluster. This objective is also known as the Within-Groups Sum of Squares or the Inertia. By minimizing this sum, K-Means aims to ensure that data points within each cluster are closer to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd99e9c",
   "metadata": {},
   "source": [
    "### (10) How can the number of clusters in a Gaussian Mixture Model (GMM) be determined in a statistically sound way?\n",
    "\n",
    "- By selecting an arbitrary number of clusters based on domain knowledge\n",
    "- By incrementally increasing the number of clusters until a satisfactory result is achieved\n",
    "- **<span style='color: red'>By using the Bayesian Information Criterion (BIC)</span>**\n",
    "- By performing hierarchical clustering and cutting the dendrogram at a specific level\n",
    "\n",
    "The number of clusters in a GMM can be determined in a statistically sound way by using the Bayesian Information Criterion (BIC). The BIC is a criterion that balances the goodness of fit of the model and the complexity of the model. By evaluating the BIC for different numbers of clusters, one can select the number of clusters that minimizes the BIC, providing a statistically sound approach for determining the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f1580e",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
