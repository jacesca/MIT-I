{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f2fd74",
   "metadata": {},
   "source": [
    "# Practice Quiz - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6cadab",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98c576",
   "metadata": {},
   "source": [
    "## Q No: 1 - Which of the following techniques is a type of Supervised Learning?\n",
    "- Clustering\n",
    "- Dimensionality Reduction\n",
    "- **Regression** ✅\n",
    "- None of the above\n",
    "\n",
    "> In Supervised Learning, the data is labeled, which means that the output variable is known for each input example. Regression is a type of supervised learning where the goal is to predict a continuous output variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9d436",
   "metadata": {},
   "source": [
    "## Q No: 2 - State whether the following statement is True or False:\n",
    "\n",
    "**In Linear Regression, the goal is to find the line of best fit that minimizes the sum of the squared residuals, where the residuals are the differences between the actual values and the predicted values of the dependent variable.**\n",
    "\n",
    "- **True** ✅\n",
    "- False\n",
    "\n",
    "> In Linear Regression, the goal is to find a line that best fits the given data points. This line is determined by minimizing the sum of the squared residuals, which are the differences between the actual observed values and the predicted values by the linear regression model. By minimizing the sum of the squared residuals, we are finding the line that has the smallest overall error or deviation from the data points, making it the best fit for the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e977f",
   "metadata": {},
   "source": [
    "## Q No: 3 - Which of the following statements is correct about overfitting in Machine Learning?\n",
    "\n",
    "- Overfitting occurs when a model is too simple and fails to capture the underlying patterns in the data\n",
    "- **Overfitting occurs when a model performs well on the training data but poorly on new, unseen data** ✅\n",
    "- Overfitting occurs when a model is trained on a large dataset\n",
    "- None of the above\n",
    "\n",
    "> * Overfitting in machine learning refers to a situation where a model becomes too complex and starts to fit the noise or random fluctuations in the training data, rather than capturing the underlying patterns or general trends. As a result, the overfitted model may have excellent performance on the training data but performs poorly when applied to new, unseen data. This happens because the model has become too specific to the training data and fails to generalize well to unseen instances. <br>\n",
    "> * Underfitting occurs when a model is too simple and unable to capture the complex patterns in the data. <br>\n",
    "> * Overfitting is not specifically related to the size of the training dataset. It can occur with both small and large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2519f28",
   "metadata": {},
   "source": [
    "## Q No: 4 - Why is regularization used in Machine Learning models?\n",
    "**A. To prevent overfitting in the model** <br>\n",
    "**B. To improve the model's ability to generalize to new, unseen data**\n",
    "\n",
    "\n",
    "- Only A\n",
    "- Only B\n",
    "- **Both A & B** ✅\n",
    "- Neither A nor B\n",
    "\n",
    "> Regularization is a technique used in machine learning to prevent overfitting, which occurs when a model becomes too complex and fits the training data too closely, leading to poor performance on new, unseen data. By adding a regularization term to the model's objective function, such as the sum of the squared magnitudes of the coefficients in Ridge regression, regularization helps to control the complexity of the model and prevent it from becoming too sensitive to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99bea79",
   "metadata": {},
   "source": [
    "## Q No: 5 - Which of the following options is used as the penalty term in Ridge Regression?\n",
    "\n",
    "- **L2 regularization term, which is the square of the magnitude of the coefficients** ✅\n",
    "- L1 regularization term, which is the absolute value of the coefficients\n",
    "- Sum of the coefficients\n",
    "- None of the above\n",
    "\n",
    "> In Ridge regression, a penalty term is added to the cost function to prevent overfitting and reduce the impact of large coefficient values. The penalty term is calculated using L2 regularization, which is the sum of the squared magnitudes of the coefficients multiplied by a regularization parameter (alpha). This term encourages the coefficients to be small, resulting in a simpler and more generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f64c8ae",
   "metadata": {},
   "source": [
    "\n",
    "## Q No: 6 - Which of the following statements accurately compares Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)?\n",
    "\n",
    "- **LDA assumes that the covariance matrices of the classes are equal, while QDA allows for different covariance matrices** ✅\n",
    "- LDA and QDA both assume that the classes have equal priors\n",
    "- LDA and QDA perform equally well regardless of the distribution of the data\n",
    "- None of the above\n",
    "\n",
    "> * LDA and QDA are both classification techniques that assume a Gaussian distribution for the data. However, they differ in their assumptions about the covariance matrices of the classes. LDA assumes that the covariance matrices are equal for all classes, which means that the decision boundary is linear.<br>\n",
    "> * On the other hand, QDA allows for different covariance matrices for each class, resulting in a quadratic decision boundary. This flexibility of QDA allows it to capture more complex relationships in the data, but it can also be more prone to overfitting when the number of features is large or when the dataset is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae036b",
   "metadata": {},
   "source": [
    "## Q No: 7 - Which of the following statements accurately describes Leave-One-Out Cross-Validation (LOOCV)?\n",
    "\n",
    "**1. In LOOCV, the model is trained using all data points except one, which is used for validation. This process is repeated for each data point in the dataset**\n",
    "**2. LOOCV eliminates the variability introduced by the random selection of a validation set because each data point serves as a validation point exactly once**\n",
    "\n",
    "- Only 1\n",
    "- Only 2\n",
    "- **Both 1 & 2** ✅\n",
    "- None of the above\n",
    "\n",
    "> * Leave-One-Out Cross-Validation (LOOCV) is a technique used to evaluate the performance of a model when the dataset is limited. In LOOCV, the model is trained and validated using a process called \"leave-one-out\" iteration.<br>\n",
    "> * In LOOCV, the model is trained on all data points except one, which is held out as the validation data. This process is repeated for each data point in the dataset. For example, if you have n data points, the model will be trained n times, each time leaving out one data point for validation.<br>\n",
    "> * LOOCV eliminates the variability introduced by randomly selecting a validation set. Since each data point serves as a validation point exactly once, there is no randomness in the selection of validation data. This ensures that the evaluation of the model's performance is not influenced by the specific choice of a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c633d",
   "metadata": {},
   "source": [
    "## Q No: 8 - Arrange the following steps of K-Fold Cross-Validation in the correct order:\n",
    "\n",
    "**1. The dataset is divided into K Folds**\n",
    "**2. Take the average across all the Folds to obtain the overall performance of the model**\n",
    "**3. Evaluate the performance of the model on the hold-out Fold**\n",
    "**4. The model is trained on K-1 Folds**\n",
    "\n",
    "- 1, 3, 2, 4\n",
    "- 4, 3, 1, 3\n",
    "- 2, 3, 4, 1\n",
    "- **4, 1, 3, 2** ✅\n",
    "\n",
    "\n",
    "> * K-fold Cross-Validation (CV) is a technique used to evaluate the performance of a machine learning model.<br>\n",
    "> * The dataset is divided into K folds. In K-fold CV, the dataset is divided into K approximately equal-sized folds or subsets. This division allows for training and evaluation of different subsets of the data. <br>\n",
    "> * The model is trained on K-1 folds of the dataset and evaluated on the remaining fold for each iteration. In each iteration of the K-fold CV, the model is trained on K-1 folds (used as training data) and evaluated on the remaining fold (used as validation or test data). This process is repeated K times, with each fold serving as the validation fold once. <br>\n",
    "> * Evaluate the error on the hold-out fold. After training the model on K-1 folds, the performance of the model is typically assessed by evaluating the error or performance metric on the hold-out fold, which is the fold not used for training in each iteration.<br>\n",
    "> * Once the above steps are done for all the folds, we take the average across all the Folds to obtain the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d733a5",
   "metadata": {},
   "source": [
    "## Q No: 9 - Which of the following metrics can be used to evaluate classification models?\n",
    "\n",
    "- Mean Absolute Error\n",
    "- Root Mean Squared Error\n",
    "- **Accuracy** ✅\n",
    "- Mean Squared Error\n",
    "\n",
    "> * Accuracy is a commonly used metric for evaluating classification models. It measures the proportion of correct predictions out of the total number of predictions made by the model. In classification tasks, accuracy tells us how well the model can classify instances correctly.\n",
    "> * Mean Absolute Error, Root Mean Squared Error, and Mean Squared Error are typically used as evaluation metrics for regression tasks rather than classification tasks. They measure the difference between the predicted and actual values in a regression model. These metrics are not directly applicable to classification problems, where the focus is on predicting discrete class labels rather than continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f7076",
   "metadata": {},
   "source": [
    "## Q No: 10 - Which of the following problems is an example of a classification problem?\n",
    "\n",
    "- Predicting house prices based on the area of the house\n",
    "- **Identifying spam emails from a given set of emails** ✅\n",
    "- Estimating the average temperature for the next day\n",
    "- Predicting the number of sales for a new product based on marketing spend\n",
    "\n",
    "> Identifying spam emails from a given set of emails is an example of a classification problem. The task involves classifying emails into two categories: spam and not spam. The goal is to develop a model that can accurately distinguish between spam and non-spam emails based on various features such as email content, sender information, and email attachments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b0619",
   "metadata": {},
   "source": [
    "----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
